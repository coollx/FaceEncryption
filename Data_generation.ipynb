{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Video\n",
    "import cv2\n",
    "import os\n",
    "import random\n",
    "\n",
    "from facenet_pytorch import MTCNN\n",
    "from PIL import Image\n",
    "import torch\n",
    "from imutils.video import FileVideoStream\n",
    "import time\n",
    "import glob\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastMTCNN(object):\n",
    "    \"\"\"Fast MTCNN implementation.\"\"\"\n",
    "    \n",
    "    def __init__(self, stride, resize=1, *args, **kwargs):\n",
    "        \"\"\"Constructor for FastMTCNN class.\n",
    "        \n",
    "        Arguments:\n",
    "            stride (int): The detection stride. Faces will be detected every `stride` frames\n",
    "                and remembered for `stride-1` frames.\n",
    "        \n",
    "        Keyword arguments:\n",
    "            resize (float): Fractional frame scaling. [default: {1}]\n",
    "            *args: Arguments to pass to the MTCNN constructor. See help(MTCNN).\n",
    "            **kwargs: Keyword arguments to pass to the MTCNN constructor. See help(MTCNN).\n",
    "        \"\"\"\n",
    "        self.stride = stride\n",
    "        self.resize = resize\n",
    "        self.mtcnn = MTCNN(*args, **kwargs)\n",
    "        \n",
    "    def __call__(self, frames):\n",
    "        \"\"\"Detect faces in frames using strided MTCNN.\"\"\"\n",
    "        if self.resize != 1:\n",
    "            frames = [\n",
    "                cv2.resize(f, (int(f.shape[1] * self.resize), int(f.shape[0] * self.resize)))\n",
    "                    for f in frames\n",
    "            ]\n",
    "                      \n",
    "        boxes, probs = self.mtcnn.detect(frames[::self.stride])\n",
    "\n",
    "        faces = []\n",
    "        for i, frame in enumerate(frames):\n",
    "            box_ind = int(i / self.stride)\n",
    "            if boxes[box_ind] is None:\n",
    "                continue\n",
    "            for box in boxes[box_ind]:\n",
    "                box = [int(b) for b in box]\n",
    "                center_x, center_y = (box[0] + box[2]) // 2, (box[1] + box[3]) // 2\n",
    "                wide = max(box[2] - box[0], box[3] - box[1]) // 2\n",
    "                face = frame[center_y - wide:center_y + wide, center_x - wide:center_x + wide]\n",
    "                faces.append(face)\n",
    "                #faces.append(frame[box[1] :box[3], box[0]:box[2]])\n",
    "                \n",
    "        \n",
    "        return faces\n",
    "    \n",
    "fast_mtcnn = FastMTCNN(\n",
    "    \n",
    "    stride=1,\n",
    "    resize=1,\n",
    "    #image_size=1000,\n",
    "    #margin=20,\n",
    "    factor=0.6,\n",
    "    keep_all=True,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\envs\\SDS\\lib\\site-packages\\facenet_pytorch\\models\\utils\\detect_face.py:250: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if method is \"Min\":\n",
      "c:\\ProgramData\\Anaconda3\\envs\\SDS\\lib\\site-packages\\facenet_pytorch\\models\\utils\\detect_face.py:250: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if method is \"Min\":\n",
      "c:\\ProgramData\\Anaconda3\\envs\\SDS\\lib\\site-packages\\facenet_pytorch\\models\\utils\\detect_face.py:250: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if method is \"Min\":\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "need at least one array to stack",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\x\\cs\\3xxx\\SDS 3386\\FaceEncryption\\Data_generation.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 41>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell://ssh-remote%2Balchemist/d%3A/x/cs/3xxx/SDS%203386/FaceEncryption/Data_generation.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=36'>37</a>\u001b[0m         cv2\u001b[39m.\u001b[39mimwrite(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m./Private_dataset/\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mformat\u001b[39m(path)\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mformat\u001b[39m(count, \u001b[39m'\u001b[39m\u001b[39m04d\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m.jpg\u001b[39m\u001b[39m\"\u001b[39m, face)\n\u001b[0;32m     <a href='vscode-notebook-cell://ssh-remote%2Balchemist/d%3A/x/cs/3xxx/SDS%203386/FaceEncryption/Data_generation.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=37'>38</a>\u001b[0m         count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell://ssh-remote%2Balchemist/d%3A/x/cs/3xxx/SDS%203386/FaceEncryption/Data_generation.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=40'>41</a>\u001b[0m generate_frame(\u001b[39m'\u001b[39;49m\u001b[39mXiang\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m40\u001b[39;49m)\n",
      "\u001b[1;32md:\\x\\cs\\3xxx\\SDS 3386\\FaceEncryption\\Data_generation.ipynb Cell 3\u001b[0m in \u001b[0;36mgenerate_frame\u001b[1;34m(path, frame_number)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell://ssh-remote%2Balchemist/d%3A/x/cs/3xxx/SDS%203386/FaceEncryption/Data_generation.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell://ssh-remote%2Balchemist/d%3A/x/cs/3xxx/SDS%203386/FaceEncryption/Data_generation.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mlen\u001b[39m(res))\n\u001b[1;32m---> <a href='vscode-notebook-cell://ssh-remote%2Balchemist/d%3A/x/cs/3xxx/SDS%203386/FaceEncryption/Data_generation.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m res \u001b[39m=\u001b[39m fast_mtcnn(res)\n\u001b[0;32m     <a href='vscode-notebook-cell://ssh-remote%2Balchemist/d%3A/x/cs/3xxx/SDS%203386/FaceEncryption/Data_generation.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mlen\u001b[39m(res))\n\u001b[0;32m     <a href='vscode-notebook-cell://ssh-remote%2Balchemist/d%3A/x/cs/3xxx/SDS%203386/FaceEncryption/Data_generation.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=34'>35</a>\u001b[0m count \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "\u001b[1;32md:\\x\\cs\\3xxx\\SDS 3386\\FaceEncryption\\Data_generation.ipynb Cell 3\u001b[0m in \u001b[0;36mFastMTCNN.__call__\u001b[1;34m(self, frames)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell://ssh-remote%2Balchemist/d%3A/x/cs/3xxx/SDS%203386/FaceEncryption/Data_generation.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresize \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m     <a href='vscode-notebook-cell://ssh-remote%2Balchemist/d%3A/x/cs/3xxx/SDS%203386/FaceEncryption/Data_generation.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m     frames \u001b[39m=\u001b[39m [\n\u001b[0;32m     <a href='vscode-notebook-cell://ssh-remote%2Balchemist/d%3A/x/cs/3xxx/SDS%203386/FaceEncryption/Data_generation.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m         cv2\u001b[39m.\u001b[39mresize(f, (\u001b[39mint\u001b[39m(f\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresize), \u001b[39mint\u001b[39m(f\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresize)))\n\u001b[0;32m     <a href='vscode-notebook-cell://ssh-remote%2Balchemist/d%3A/x/cs/3xxx/SDS%203386/FaceEncryption/Data_generation.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m             \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m frames\n\u001b[0;32m     <a href='vscode-notebook-cell://ssh-remote%2Balchemist/d%3A/x/cs/3xxx/SDS%203386/FaceEncryption/Data_generation.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m     ]\n\u001b[1;32m---> <a href='vscode-notebook-cell://ssh-remote%2Balchemist/d%3A/x/cs/3xxx/SDS%203386/FaceEncryption/Data_generation.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m boxes, probs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmtcnn\u001b[39m.\u001b[39;49mdetect(frames[::\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride])\n\u001b[0;32m     <a href='vscode-notebook-cell://ssh-remote%2Balchemist/d%3A/x/cs/3xxx/SDS%203386/FaceEncryption/Data_generation.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m faces \u001b[39m=\u001b[39m []\n\u001b[0;32m     <a href='vscode-notebook-cell://ssh-remote%2Balchemist/d%3A/x/cs/3xxx/SDS%203386/FaceEncryption/Data_generation.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, frame \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(frames):\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\SDS\\lib\\site-packages\\facenet_pytorch\\models\\mtcnn.py:313\u001b[0m, in \u001b[0;36mMTCNN.detect\u001b[1;34m(self, img, landmarks)\u001b[0m\n\u001b[0;32m    273\u001b[0m \u001b[39m\"\"\"Detect all faces in PIL image and return bounding boxes and optional facial landmarks.\u001b[39;00m\n\u001b[0;32m    274\u001b[0m \n\u001b[0;32m    275\u001b[0m \u001b[39mThis method is used by the forward method and is also useful for face detection tasks\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    309\u001b[0m \u001b[39m>>> img_draw.save('annotated_faces.png')\u001b[39;00m\n\u001b[0;32m    310\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    312\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m--> 313\u001b[0m     batch_boxes, batch_points \u001b[39m=\u001b[39m detect_face(\n\u001b[0;32m    314\u001b[0m         img, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmin_face_size,\n\u001b[0;32m    315\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpnet, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrnet, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49monet,\n\u001b[0;32m    316\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mthresholds, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfactor,\n\u001b[0;32m    317\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice\n\u001b[0;32m    318\u001b[0m     )\n\u001b[0;32m    320\u001b[0m boxes, probs, points \u001b[39m=\u001b[39m [], [], []\n\u001b[0;32m    321\u001b[0m \u001b[39mfor\u001b[39;00m box, point \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(batch_boxes, batch_points):\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\SDS\\lib\\site-packages\\facenet_pytorch\\models\\utils\\detect_face.py:40\u001b[0m, in \u001b[0;36mdetect_face\u001b[1;34m(imgs, minsize, pnet, rnet, onet, threshold, factor, device)\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39many\u001b[39m(img\u001b[39m.\u001b[39msize \u001b[39m!=\u001b[39m imgs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39msize \u001b[39mfor\u001b[39;00m img \u001b[39min\u001b[39;00m imgs):\n\u001b[0;32m     39\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mMTCNN batch processing only compatible with equal-dimension images.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 40\u001b[0m     imgs \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mstack([np\u001b[39m.\u001b[39;49muint8(img) \u001b[39mfor\u001b[39;49;00m img \u001b[39min\u001b[39;49;00m imgs])\n\u001b[0;32m     41\u001b[0m     imgs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mas_tensor(imgs\u001b[39m.\u001b[39mcopy(), device\u001b[39m=\u001b[39mdevice)\n\u001b[0;32m     45\u001b[0m model_dtype \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(pnet\u001b[39m.\u001b[39mparameters())\u001b[39m.\u001b[39mdtype\n",
      "File \u001b[1;32m<__array_function__ internals>:5\u001b[0m, in \u001b[0;36mstack\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\SDS\\lib\\site-packages\\numpy\\core\\shape_base.py:422\u001b[0m, in \u001b[0;36mstack\u001b[1;34m(arrays, axis, out)\u001b[0m\n\u001b[0;32m    420\u001b[0m arrays \u001b[39m=\u001b[39m [asanyarray(arr) \u001b[39mfor\u001b[39;00m arr \u001b[39min\u001b[39;00m arrays]\n\u001b[0;32m    421\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m arrays:\n\u001b[1;32m--> 422\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mneed at least one array to stack\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    424\u001b[0m shapes \u001b[39m=\u001b[39m {arr\u001b[39m.\u001b[39mshape \u001b[39mfor\u001b[39;00m arr \u001b[39min\u001b[39;00m arrays}\n\u001b[0;32m    425\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(shapes) \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n",
      "\u001b[1;31mValueError\u001b[0m: need at least one array to stack"
     ]
    }
   ],
   "source": [
    "def generate_frame(path, frame_number):\n",
    "    #create folder if it doesn't exist\n",
    "    if not os.path.exists('Private_dataset/' + path):\n",
    "        os.makedirs('Private_dataset/' + path)\n",
    "        \n",
    "\n",
    "    vidcap = cv2.VideoCapture('Private_dataset/{}.MOV'.format(path))\n",
    "\n",
    "    #sample 40 frames uniformly from the video\n",
    "    frame_count = int(vidcap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    frame_indices = sorted(random.sample(range(frame_count), frame_number))\n",
    "    vidcap.set(cv2.CAP_PROP_POS_FRAMES, frame_indices[0])\n",
    "    success, image = vidcap.read()\n",
    "    count = 0\n",
    "    res = []\n",
    "    \n",
    "    for i in range(1, frame_count):\n",
    "        if i == frame_indices[count]:\n",
    "            #rotate the image 180 degrees\n",
    "            image = cv2.rotate(image, cv2.ROTATE_180)\n",
    "            # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            # faces = fast_mtcnn(image)\n",
    "            # cv2.imwrite(f\"./Private_dataset/{format(path)}/{format(count, '04d')}.jpg\", image)\n",
    "            res.append(image)\n",
    "            count += 1\n",
    "            if count >= len(frame_indices):\n",
    "                break\n",
    "        success, image = vidcap.read()\n",
    "        if not success:\n",
    "            break\n",
    "    \n",
    "    print(len(res))\n",
    "    res = fast_mtcnn(res)\n",
    "    print(len(res))\n",
    "    count = 0\n",
    "    for face in res:\n",
    "        cv2.imwrite(f\"./Private_dataset/{format(path)}/{format(count, '04d')}.jpg\", face)\n",
    "        count += 1\n",
    "\n",
    "\n",
    "generate_frame('Xiang', 40)\n",
    "# generate_frame('Jiaxun', 40)\n",
    "# generate_frame('Bowen', 40)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('SDS')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "64d289be222cedc063c7d93f6e3cd98c637b22e21817e88681cd2d85cbb06b17"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
